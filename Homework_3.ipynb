{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c54d3a5-9353-40fe-8c38-05e18404c2ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework_3\n",
    "## Real Time Machine Learning\n",
    "### Authors: Mark McAfoose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f79c1f6-bb0d-4b3e-a0b4-059a3fb2d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import collections\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a965c7fc-197a-4190-a62c-7ff0d8ff22f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12f0e02d350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) # Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32009a35-4fca-4ec7-9a65-4dba87e5869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? True\n",
      "Device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU to run on:\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if gpu_avail:\n",
    "    print(\"Device name: \" + torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9731dc14-62da-4a64-9fff-0312f76e4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bff758-c916-42fe-8520-3d80fbf90cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data mean [0.49139968 0.48215841 0.44653091]\n",
      "Data std [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "#Load dataset, calculate mean and std.dev\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
    "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
    "print(\"Data mean\", DATA_MEANS)\n",
    "print(\"Data std\", DATA_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5a2ba3-cc91-47a3-b2bf-19ad4a792d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# Splits dataset into a training and validation part\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "set_seed(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "set_seed(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de2a53-38ef-4e4e-9896-be52fbf238d1",
   "metadata": {},
   "source": [
    "# Question 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9152e777-ff34-41c8-967d-fea0d8b4c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d53793c-3085-477b-972d-6a9b7dcb19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and validation functions for first model:\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "            \n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
    "                total += labels.shape[0]  # <3>\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e244b81b-7609-4804-8644-9cdf5ee14603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18354 [432, 16, 1152, 8, 16384, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516e47048595416fa6dff70fc188650c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 22:05:34.450223 Epoch 1, Training loss 2.273497218080396\n",
      "2022-03-30 22:07:10.810542 Epoch 10, Training loss 1.4329034276837298\n",
      "2022-03-30 22:09:00.378298 Epoch 20, Training loss 1.2452135082663294\n",
      "2022-03-30 22:10:47.866595 Epoch 30, Training loss 1.1421731885342177\n",
      "2022-03-30 22:12:38.284559 Epoch 40, Training loss 1.079759429323028\n",
      "2022-03-30 22:14:31.892217 Epoch 50, Training loss 1.0376402426649023\n",
      "2022-03-30 22:16:22.786262 Epoch 60, Training loss 1.008172811266364\n",
      "2022-03-30 22:18:10.137999 Epoch 70, Training loss 0.9806953592178149\n",
      "2022-03-30 22:19:59.618725 Epoch 80, Training loss 0.9628937067808928\n",
      "2022-03-30 22:21:49.382777 Epoch 90, Training loss 0.9444924730520982\n",
      "2022-03-30 22:23:45.611593 Epoch 100, Training loss 0.9317056480635945\n",
      "2022-03-30 22:25:35.065324 Epoch 110, Training loss 0.9175461669932743\n",
      "2022-03-30 22:27:27.563625 Epoch 120, Training loss 0.9061607076571538\n",
      "2022-03-30 22:29:15.596610 Epoch 130, Training loss 0.9012549086513683\n",
      "2022-03-30 22:31:01.748596 Epoch 140, Training loss 0.8903468784443673\n",
      "2022-03-30 22:32:50.439143 Epoch 150, Training loss 0.8828292838868252\n",
      "2022-03-30 22:34:37.849861 Epoch 160, Training loss 0.8696994852815938\n",
      "2022-03-30 22:36:25.520178 Epoch 170, Training loss 0.8658463421710196\n",
      "2022-03-30 22:38:12.862917 Epoch 180, Training loss 0.8567507686438384\n",
      "2022-03-30 22:40:00.427942 Epoch 190, Training loss 0.8500163792884587\n",
      "2022-03-30 22:41:49.109982 Epoch 200, Training loss 0.8449021844442753\n",
      "2022-03-30 22:43:36.279774 Epoch 210, Training loss 0.8402009168241777\n",
      "2022-03-30 22:45:22.634960 Epoch 220, Training loss 0.8324727793025155\n",
      "2022-03-30 22:47:08.998124 Epoch 230, Training loss 0.8269156755884828\n",
      "2022-03-30 22:48:55.331968 Epoch 240, Training loss 0.8243892449259418\n",
      "2022-03-30 22:50:42.047683 Epoch 250, Training loss 0.8164818816714816\n",
      "2022-03-30 22:52:29.089582 Epoch 260, Training loss 0.8189234840564239\n",
      "2022-03-30 22:54:14.243063 Epoch 270, Training loss 0.8156073351531287\n",
      "2022-03-30 22:56:01.810358 Epoch 280, Training loss 0.8096652041133653\n",
      "2022-03-30 22:57:48.996593 Epoch 290, Training loss 0.8045601931392637\n",
      "2022-03-30 22:59:34.154339 Epoch 300, Training loss 0.801543271609521\n",
      "Accuracy train: 0.73\n",
      "Accuracy val: 0.70\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "training_loop(  \n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fecb46-5400-4ee9-9b5c-7d58e9e8ffbd",
   "metadata": {},
   "source": [
    "# Question 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bffb3a-f50e-4571-88ca-1cae2430cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 4, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * 4)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e169ea9-fce0-4e97-9a35-6fd2bb7dd825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4310 [432, 16, 1152, 8, 288, 4, 2048, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bad99e93f44a9c8052dfc7b5d0fa55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:00:03.329929 Epoch 1, Training loss 2.3039929004136654\n",
      "2022-03-30 23:01:40.786940 Epoch 10, Training loss 1.836396298517189\n",
      "2022-03-30 23:03:26.566072 Epoch 20, Training loss 1.4520645070279765\n",
      "2022-03-30 23:05:11.793571 Epoch 30, Training loss 1.3451161907609033\n",
      "2022-03-30 23:06:58.728257 Epoch 40, Training loss 1.2806930497840598\n",
      "2022-03-30 23:08:47.000379 Epoch 50, Training loss 1.2359161472048854\n",
      "2022-03-30 23:10:34.962889 Epoch 60, Training loss 1.2019485131627814\n",
      "2022-03-30 23:12:20.853804 Epoch 70, Training loss 1.174003569828479\n",
      "2022-03-30 23:14:06.126091 Epoch 80, Training loss 1.1539142714266764\n",
      "2022-03-30 23:15:54.171997 Epoch 90, Training loss 1.13521216839467\n",
      "2022-03-30 23:17:42.008857 Epoch 100, Training loss 1.1179003751176035\n",
      "2022-03-30 23:19:30.021262 Epoch 110, Training loss 1.0906687572471097\n",
      "2022-03-30 23:21:15.296038 Epoch 120, Training loss 1.080829423207503\n",
      "2022-03-30 23:22:59.483568 Epoch 130, Training loss 1.0675467198390907\n",
      "2022-03-30 23:24:44.183920 Epoch 140, Training loss 1.05739565758284\n",
      "2022-03-30 23:26:28.392456 Epoch 150, Training loss 1.0473749433827197\n",
      "2022-03-30 23:28:12.545213 Epoch 160, Training loss 1.0423639837832872\n",
      "2022-03-30 23:29:56.485635 Epoch 170, Training loss 1.0299050536250796\n",
      "2022-03-30 23:31:40.879212 Epoch 180, Training loss 1.0309739936450948\n",
      "2022-03-30 23:33:25.395545 Epoch 190, Training loss 1.0225818938339537\n",
      "2022-03-30 23:35:09.805126 Epoch 200, Training loss 1.0164114055130895\n",
      "2022-03-30 23:36:53.795611 Epoch 210, Training loss 1.0106946394654082\n",
      "2022-03-30 23:38:37.775702 Epoch 220, Training loss 1.0120834074808323\n",
      "2022-03-30 23:40:24.604392 Epoch 230, Training loss 1.0057551499105926\n",
      "2022-03-30 23:42:12.743041 Epoch 240, Training loss 1.0035266438101091\n",
      "2022-03-30 23:44:00.682187 Epoch 250, Training loss 0.9975580559496866\n"
     ]
    }
   ],
   "source": [
    "model = NetB()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = NetB().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "training_loop(  \n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a9c85-f194-4b2b-9637-659ffe126532",
   "metadata": {},
   "source": [
    "# Question 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afaf000-de97-4422-b4ac-85e0c5c696f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        #torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = torch.relu(out)\n",
    "        return out + x #Skip connection\n",
    "\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49cf85-93c3-4829-9344-259025c8c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet10()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bfb9a9-f34b-4cf3-85a6-fa62ac3139e7",
   "metadata": {},
   "source": [
    "# Question 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e3196-af53-4a43-97c0-56a4c1acc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight Decay training loop:\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "    \n",
    "#Dropout versions:\n",
    "class ResBlock_DO(nn.Module):\n",
    "    def __init__(self, n_chans, p):\n",
    "        super(ResBlock_DO, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p = p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "\n",
    "class ResNet10_DO(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10, p=0.3):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock_DO(n_chans=n_chans1, p=p)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "#Batch Normalization versions:\n",
    "class ResBlock_BN(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock_BN, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "\n",
    "class ResNet10_BN(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock_BN(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74458527-8877-4f6b-9753-828a4e1fcb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight Decay Model Results:\n",
    "model = ResNet10()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20512e5e-f438-4bfe-a5ef-d3b149f6552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout Model Results:\n",
    "model = ResNet10_DO()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10_DO().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbea25-5deb-4e32-968c-7ebfb3d3a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Norm Model Results:\n",
    "model = ResNet10_BN()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10_BN().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
