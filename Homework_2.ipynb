{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f2bb37-a6b9-4400-b7d6-a5008f1b3a27",
   "metadata": {},
   "source": [
    "# Homework_2\n",
    "## Real Time Machine Learning\n",
    "### Authors: Mark McAfoose\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240459ac-1594-4b3c-88fd-0768177baaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6979d57-7d87-4365-9d8e-fa4d9fc36370",
   "metadata": {},
   "source": [
    "# Question 1 Pre Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e143ab05-8e65-4530-aebc-b10b3b1ff558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, in_train, in_val, out_train, out_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        p_train = model(in_train) # <1>\n",
    "        loss_train = loss_fn(p_train, out_train)\n",
    "\n",
    "        p_val = model(in_val) # <1>\n",
    "        loss_val = loss_fn(p_val, out_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(datetime.datetime.now(), f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss {loss_val.item():.4f}\")\n",
    "            \n",
    "def binary_map(x):\n",
    "    return x.map({'yes':1,\"no\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f24fd9f-21e0-42bf-8815-082c609f5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "housing = pd.DataFrame(pd.read_csv(\"./Housing.csv\")) \n",
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'price']\n",
    "varList=['mainroad','guestroom','basement','hotwaterheating','airconditioning', 'prefarea']\n",
    "input_size = len(num_vars)-1\n",
    "\n",
    "housing[varList] = housing[varList].apply(binary_map)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "#Split data into training and validation sets\n",
    "df_train, df_test = train_test_split(housing, train_size=0.8, test_size=0.2, random_state=seed)\n",
    "\n",
    "df_Newtrain = df_train[num_vars]\n",
    "df_Newtest = df_test[num_vars]\n",
    "\n",
    "# scaling the data\n",
    "df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])\n",
    "df_Newtest[num_vars] = scaler.fit_transform(df_Newtest[num_vars])\n",
    "\n",
    "#Create input and output arrays for both training and validation\n",
    "out_Newtrain = df_Newtrain.pop('price')\n",
    "in_Newtrain = df_Newtrain\n",
    "out_Newtest = df_Newtest.pop('price')\n",
    "in_Newtest = df_Newtest\n",
    "\n",
    "# convert the data to tensors\n",
    "in_train = torch.tensor(in_Newtrain.values).float()\n",
    "in_val = torch.tensor(in_Newtest.values).float()\n",
    "out_train = torch.tensor(out_Newtrain.values).float().unsqueeze(-1)\n",
    "out_val = torch.tensor(out_Newtest.values).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcac88-58c2-43ef-8fec-997c9ee6e5c9",
   "metadata": {},
   "source": [
    "# Question 1 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64aa3021-c922-4e83-a0c5-b0063d0ce0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 23:23:06.606076 Epoch 1, Training loss 1.0614, Validation loss 1.0826\n",
      "2022-03-05 23:23:06.610077 Epoch 10, Training loss 1.0414, Validation loss 1.0613\n",
      "2022-03-05 23:23:06.614077 Epoch 20, Training loss 1.0198, Validation loss 1.0385\n",
      "2022-03-05 23:23:06.618078 Epoch 30, Training loss 0.9990, Validation loss 1.0166\n",
      "2022-03-05 23:23:06.622079 Epoch 40, Training loss 0.9790, Validation loss 0.9953\n",
      "2022-03-05 23:23:06.626080 Epoch 50, Training loss 0.9595, Validation loss 0.9748\n",
      "2022-03-05 23:23:06.630081 Epoch 60, Training loss 0.9408, Validation loss 0.9549\n",
      "2022-03-05 23:23:06.634082 Epoch 70, Training loss 0.9226, Validation loss 0.9357\n",
      "2022-03-05 23:23:06.642084 Epoch 80, Training loss 0.9049, Validation loss 0.9171\n",
      "2022-03-05 23:23:06.646085 Epoch 90, Training loss 0.8878, Validation loss 0.8990\n",
      "2022-03-05 23:23:06.650085 Epoch 100, Training loss 0.8712, Validation loss 0.8815\n",
      "2022-03-05 23:23:06.654086 Epoch 110, Training loss 0.8551, Validation loss 0.8645\n",
      "2022-03-05 23:23:06.658087 Epoch 120, Training loss 0.8394, Validation loss 0.8480\n",
      "2022-03-05 23:23:06.662089 Epoch 130, Training loss 0.8242, Validation loss 0.8319\n",
      "2022-03-05 23:23:06.666089 Epoch 140, Training loss 0.8094, Validation loss 0.8164\n",
      "2022-03-05 23:23:06.670090 Epoch 150, Training loss 0.7950, Validation loss 0.8012\n",
      "2022-03-05 23:23:06.674091 Epoch 160, Training loss 0.7810, Validation loss 0.7865\n",
      "2022-03-05 23:23:06.679092 Epoch 170, Training loss 0.7674, Validation loss 0.7722\n",
      "2022-03-05 23:23:06.685093 Epoch 180, Training loss 0.7541, Validation loss 0.7583\n",
      "2022-03-05 23:23:06.689095 Epoch 190, Training loss 0.7412, Validation loss 0.7448\n",
      "2022-03-05 23:23:06.696096 Epoch 200, Training loss 0.7286, Validation loss 0.7316\n"
     ]
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(input_size, 8), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1)) #output layer\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 200, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    in_train = in_train,\n",
    "    in_val = in_val, \n",
    "    out_train = out_train,\n",
    "    out_val = out_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ed763-81d8-402a-9ce1-cb1dbfb63390",
   "metadata": {},
   "source": [
    "# Question 1 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4ac4a6-a8b8-4931-ae23-edd0fc282ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 23:23:06.712100 Epoch 1, Training loss 1.5664, Validation loss 1.5722\n",
      "2022-03-05 23:23:06.724103 Epoch 10, Training loss 1.5380, Validation loss 1.5439\n",
      "2022-03-05 23:23:06.739106 Epoch 20, Training loss 1.5080, Validation loss 1.5140\n",
      "2022-03-05 23:23:06.747108 Epoch 30, Training loss 1.4796, Validation loss 1.4858\n",
      "2022-03-05 23:23:06.757110 Epoch 40, Training loss 1.4527, Validation loss 1.4591\n",
      "2022-03-05 23:23:06.770114 Epoch 50, Training loss 1.4273, Validation loss 1.4338\n",
      "2022-03-05 23:23:06.777114 Epoch 60, Training loss 1.4032, Validation loss 1.4098\n",
      "2022-03-05 23:23:06.784117 Epoch 70, Training loss 1.3804, Validation loss 1.3871\n",
      "2022-03-05 23:23:06.798119 Epoch 80, Training loss 1.3587, Validation loss 1.3656\n",
      "2022-03-05 23:23:06.805121 Epoch 90, Training loss 1.3382, Validation loss 1.3452\n",
      "2022-03-05 23:23:06.816123 Epoch 100, Training loss 1.3187, Validation loss 1.3259\n",
      "2022-03-05 23:23:06.826126 Epoch 110, Training loss 1.3002, Validation loss 1.3075\n",
      "2022-03-05 23:23:06.834128 Epoch 120, Training loss 1.2827, Validation loss 1.2901\n",
      "2022-03-05 23:23:06.841129 Epoch 130, Training loss 1.2660, Validation loss 1.2736\n",
      "2022-03-05 23:23:06.850132 Epoch 140, Training loss 1.2502, Validation loss 1.2580\n",
      "2022-03-05 23:23:06.857133 Epoch 150, Training loss 1.2352, Validation loss 1.2431\n",
      "2022-03-05 23:23:06.871136 Epoch 160, Training loss 1.2209, Validation loss 1.2289\n",
      "2022-03-05 23:23:06.880139 Epoch 170, Training loss 1.2073, Validation loss 1.2155\n",
      "2022-03-05 23:23:06.887140 Epoch 180, Training loss 1.1944, Validation loss 1.2028\n",
      "2022-03-05 23:23:06.897142 Epoch 190, Training loss 1.1822, Validation loss 1.1906\n",
      "2022-03-05 23:23:06.907144 Epoch 200, Training loss 1.1705, Validation loss 1.1791\n"
     ]
    }
   ],
   "source": [
    "seq_model_2 = nn.Sequential(\n",
    "            nn.Linear(input_size, 8), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 4), #hidden layer 2\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 2), #hidden layer 3\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, 1)) #output layer\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(seq_model_2.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 200, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model_2,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    in_train = in_train,\n",
    "    in_val = in_val, \n",
    "    out_train = out_train,\n",
    "    out_val = out_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c1990-6460-4c63-af23-a70cd780b900",
   "metadata": {},
   "source": [
    "# Question 2 Pre Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1734207-4433-4df6-8d70-065ad3beb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Load and preprocess dataset\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_train = [(img, label) for img, label in cifar10]\n",
    "cifar10_test = [(img, label) for img, label in cifar10_val]\n",
    "\n",
    "#Use GPU if available\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7b581-bf9a-46df-a00a-b9a70964d238",
   "metadata": {},
   "source": [
    "# Question 2 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a279b2e-2423-47f7-bce0-00d1bccd1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 23:23:23.085878 Epoch 0, Training loss 2.101949691772461\n",
      "2022-03-05 23:23:43.512543 Epoch 10, Training loss 1.612116813659668\n",
      "2022-03-05 23:23:53.408569 Epoch 20, Training loss 1.585538625717163\n",
      "2022-03-05 23:24:03.936954 Epoch 30, Training loss 0.4230346977710724\n",
      "2022-03-05 23:24:20.225934 Epoch 40, Training loss 0.5442031621932983\n",
      "2022-03-05 23:24:44.915783 Epoch 50, Training loss 0.24527835845947266\n",
      "2022-03-05 23:25:06.262392 Epoch 60, Training loss 0.376009464263916\n",
      "2022-03-05 23:25:27.259402 Epoch 70, Training loss 0.07200481742620468\n",
      "2022-03-05 23:25:50.407587 Epoch 80, Training loss 0.03949423134326935\n",
      "2022-03-05 23:26:03.049776 Epoch 90, Training loss 0.034284189343452454\n",
      "2022-03-05 23:26:17.049218 Epoch 100, Training loss 0.060630038380622864\n",
      "2022-03-05 23:26:28.764108 Epoch 110, Training loss 0.03002581186592579\n",
      "2022-03-05 23:26:48.933494 Epoch 120, Training loss 0.033986691385507584\n",
      "2022-03-05 23:27:12.507886 Epoch 130, Training loss 0.019289325922727585\n",
      "2022-03-05 23:27:32.333842 Epoch 140, Training loss 0.011249490082263947\n",
      "2022-03-05 23:27:41.443412 Epoch 150, Training loss 0.015875045210123062\n",
      "2022-03-05 23:27:53.913237 Epoch 160, Training loss 0.015387553721666336\n",
      "2022-03-05 23:28:14.290420 Epoch 170, Training loss 0.012734595686197281\n",
      "2022-03-05 23:28:40.535693 Epoch 180, Training loss 0.014547576196491718\n",
      "2022-03-05 23:29:01.483451 Epoch 190, Training loss 0.014066992327570915\n",
      "2022-03-05 23:29:10.615026 Epoch 200, Training loss 0.009154916740953922\n",
      "2022-03-05 23:29:22.432807 Epoch 210, Training loss 0.005986914038658142\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "\n",
    "model_cifar = nn.Sequential(\n",
    "            nn.Linear(3072, 512), #Hidden Layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 10)) #Output layer\n",
    "\n",
    "model_cifar.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_cifar.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 301\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "      print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss)) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Train Accuracy: %f\" % (correct / total))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Val Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e7609-ad44-45d5-b6be-0155fc864616",
   "metadata": {},
   "source": [
    "# Could not figure out part 2 for the life of me\n",
    "# If you could convincce the professor to go over this homework in great detail after Spring Break, I would greatly appreciate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca42c23-5c6e-4d3a-8695-267246a51c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
